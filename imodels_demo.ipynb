{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKb-uLnXorLq"
      },
      "source": [
        "## AIPI 590 - XAI | Assignment #4\n",
        "### Interpretable ML II\n",
        "#### Author: Tal Erez\n",
        "#### Colab Notebook:\n",
        "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/notthattal/InterpretableML_II/blob/main/imodels_demo.ipynb)\n",
        "\n",
        "### Introduction\n",
        "\n",
        "This notebook demonstrates the CART, FIGS and Rule-Fit algorithms which can be found [here](https://github.com/csinva/imodels?tab=readme-ov-file) and are part of Python's imodels interpretability library.\n",
        "\n",
        "### Install required dependencies and import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4K2liD0qorLr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Remove Colab default sample_data if it exists\n",
        "!rm -r sample_data\n",
        "\n",
        "# Clone GitHub files to colab workspace\n",
        "repo_name = \"InterpretableML_II\"\n",
        "\n",
        "# Check if the repo already exists\n",
        "if not os.path.exists(\"/content/\" + repo_name):\n",
        "    git_path = 'https://github.com/notthattal/InterpretableML_II.git'\n",
        "    !git clone \"{git_path}\"\n",
        "else:\n",
        "    print(f\"{repo_name} already exists.\")\n",
        "\n",
        "# Change working directory to location of notebook\n",
        "path_to_notebook = os.path.join(\"/content/\" + repo_name)\n",
        "%cd \"{path_to_notebook}\"\n",
        "%ls\n",
        "\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "from imodels import get_clean_dataset, GreedyTreeClassifier, FIGSClassifier, RuleFitRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris, load_breast_cancer\n",
        "from sklearn.tree import plot_tree\n",
        "from sklearn.metrics import classification_report, mean_squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p11gx3KoorLr"
      },
      "source": [
        "### Load Iris Dataset\n",
        "- loads the dataset\n",
        "- assign the features and target\n",
        "- assign the feature names and target names\n",
        "- split the data into training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IOlx5a0orLr"
      },
      "outputs": [],
      "source": [
        "# Load the iris dataset from sklearn\n",
        "dataset = load_iris()\n",
        "\n",
        "# assign the features and target\n",
        "X = dataset.data\n",
        "y = dataset.target\n",
        "\n",
        "# assign the feature and target names\n",
        "feature_names = dataset.feature_names\n",
        "target_names = dataset.target_names\n",
        "\n",
        "# split the data into a training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-DuH95oorLr"
      },
      "source": [
        "## CART (Classification And Regression Tree)\n",
        "The Cart Approach:\n",
        "1. CART works by using a greedy approach for splitting. Different splitting points are tested using a cost function (generally SSE for regression and Gini Index for classification)\n",
        "2. The algorithm chooses to split the node based on the minimum cost\n",
        "3. Repeat the process until the stopping criterion is reached. The most common stopping criterion used for cart is a threshold representing a minimum amount of training data for every leaf node.\n",
        "4. It is recommended to Prune the tree before outputting the final model, but it is not required.\n",
        "5. Output the final tree\n",
        "\n",
        "![CART Image](https://github.com/notthattal/InterpretableML_II/blob/main/img/CART.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQkCUzSSorLr"
      },
      "source": [
        "### CART Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSyqrVkvorLr"
      },
      "outputs": [],
      "source": [
        "# initialize the CART classifier\n",
        "model = GreedyTreeClassifier(max_depth=3)\n",
        "\n",
        "#fit the model\n",
        "model.fit(X_train, y_train, feature_names=feature_names)\n",
        "\n",
        "# get the predictions\n",
        "preds = model.predict(X_test)\n",
        "\n",
        "# create and display the plot of the created tree\n",
        "plt.figure(figsize=(15, 10))\n",
        "plot_tree(model, filled=True, feature_names=feature_names, class_names=target_names)\n",
        "plt.show()\n",
        "\n",
        "# display metrics\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, preds, target_names=target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlumo2ZtorLs"
      },
      "source": [
        "### Load Heart Dataset\n",
        "- loads the dataset\n",
        "- assign the features and target\n",
        "- assign the feature names and target names\n",
        "- split the data into training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vK5thjwDorLs"
      },
      "outputs": [],
      "source": [
        "# get the features, target and feature names from imodels' heart dataset\n",
        "X, y, feature_names = get_clean_dataset('heart')\n",
        "\n",
        "# split the data into a training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kib7XETaorLs"
      },
      "source": [
        "### FIGS (Fast Interpretable Greedy-Tree Sums)\n",
        "The FIGS Approach:\n",
        "1. Given a dataset, at each iteration of FIGS there is the option to split a current tree or create a new tree. The choice to split or create\n",
        "a new tree is either based on whichever minimizes the total unexplained variance or another splitting criterion.\n",
        "2. After splitting (or creating the new tree), the model predicts the residuals for each tree after summing the predictions over all other trees.\n",
        "3. This process is repeated until a stopping criterion is met. Some common stopping criterions are:\n",
        "    - A threshold based on the model's predictive performance\n",
        "    - Domain knowledge on how interpretable the model needs to be\n",
        "    - Impurity decrease threshold\n",
        "\n",
        "![FIGS Image](https://github.com/notthattal/InterpretableML_II/blob/main/img/FIGS.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxJIFt7-orLs"
      },
      "source": [
        "### FIGS Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEoA30DLorLs"
      },
      "outputs": [],
      "source": [
        "# initialize the FIGS Classifier\n",
        "model = FIGSClassifier(max_rules=4)\n",
        "\n",
        "# fit the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# visualize the model\n",
        "model.plot(feature_names=feature_names, fig_size=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doQyjWAnorLs"
      },
      "source": [
        "### Load Breast Cancer Dataset\n",
        "- loads the dataset\n",
        "- assign the features and target\n",
        "- assign the feature names and target names\n",
        "- split the data into training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zgKZWf4orLs"
      },
      "outputs": [],
      "source": [
        "# Load the breast cancer dataset from sklearn\n",
        "dataset = load_breast_cancer()\n",
        "\n",
        "# get the features, target, feature names and target names from the breast cancer dataset\n",
        "X = dataset.data\n",
        "y = dataset.target\n",
        "feature_names = dataset.feature_names\n",
        "target_names = dataset.target_names\n",
        "\n",
        "# split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo6DzC-BorLs"
      },
      "source": [
        "### Rule-Fit Regression\n",
        "The Rule-Fit Approach:\n",
        "1. Create a decision-tree ensemble based on a dataset\n",
        "2. The rulefit algorithm creates a rule (which is binary) for each node of every tree in the ensemble\n",
        "3. Once rules are generated they are added as new features to the original dataset\n",
        "4. We then fit the new data on a LASSO regularized linear regression model which accounts for the interaction features\n",
        "\n",
        "![RuleFit Image](https://github.com/notthattal/InterpretableML_II/blob/main/img/RuleFit.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZN-7g9_orLs"
      },
      "source": [
        "### Rule-Fit Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hIk3q03orLs"
      },
      "outputs": [],
      "source": [
        "# initialize the Rule-Fit Regressor\n",
        "model = RuleFitRegressor(random_state=42)\n",
        "\n",
        "# fit the model\n",
        "model.fit(X_train, y_train, feature_names=feature_names)\n",
        "\n",
        "# get the predictions on the test set\n",
        "preds = model.predict(X_test)\n",
        "\n",
        "# calculate and display the mean-squared error\n",
        "mse = mean_squared_error(y_test, preds)\n",
        "print(f'Mean Squared Error: {mse:.2f}\\n')\n",
        "\n",
        "# visualize the rules created by the model\n",
        "model.visualize()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}